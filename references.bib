@inproceedings{catto2005iterative,
  title={Iterative dynamics with temporal coherence},
  author={Catto, Erin},
  booktitle={Game developer conference},
  volume={2},
  number={5},
  year={2005}
}

@misc{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2023-02-22},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arxiv},
  file = {/home/yuji/Dropbox/Zotero/arXiv2017/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf}
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@article{richardsFlexibleGrowthFunction1959,
  title = {A {{Flexible Growth Function}} for {{Empirical Use}}},
  author = {Richards, F. J.},
  year = {1959},
  month = jun,
  journal = {Journal of Experimental Botany},
  volume = {10},
  number = {2},
  pages = {290--301},
  issn = {0022-0957},
  doi = {10.1093/jxb/10.2.290},
  urldate = {2023-02-28},
  abstract = {The application of an extended form of von Bertalanffy's growth function to plant data is considered; the equation has considerable flexibility, but is used only to supply an empirical fit. In order to aid the biological analysis of such growth data as are capable of representation by the function, general rate parameters are deduced which are related in a simple manner to its constants.},
  file = {/home/yuji/Dropbox/Zotero/Journal of Experimental Botany/1959/Richards_1959_A Flexible Growth Function for Empirical Use.pdf}
}

@article{kirkwoodDecipheringDeathCommentary2015,
  title = {Deciphering Death: A Commentary on {{Gompertz}} (1825) `{{On}} the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies'},
  shorttitle = {Deciphering Death},
  author = {Kirkwood, Thomas B. L.},
  year = {2015},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {370},
  number = {1666},
  pages = {20140379},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2014.0379},
  urldate = {2023-03-01},
  abstract = {In 1825, the actuary Benjamin Gompertz read a paper, `On the nature of the function expressive of the law of human mortality, and on a new mode of determining the value of life contingencies', to the Royal Society in which he showed that over much of the adult human lifespan, age-specific mortality rates increased in an exponential manner. Gompertz's work played an important role in shaping the emerging statistical science that underpins the pricing of life insurance and annuities. Latterly, as the subject of ageing itself became the focus of scientific study, the Gompertz model provided a powerful stimulus to examine the patterns of death across the life course not only in humans but also in a wide range of other organisms. The idea that the Gompertz model might constitute a fundamental `law of mortality' has given way to the recognition that other patterns exist, not only across the species range but also in advanced old age. Nevertheless, Gompertz's way of representing the function expressive of the pattern of much of adult mortality retains considerable relevance for studying the factors that influence the intrinsic biology of ageing. This commentary was written to celebrate the 350th anniversary of the journal Philosophical Transactions of the Royal Society.},
  file = {/home/yuji/Dropbox/Zotero/Philosophical Transactions of the Royal Society B Biological Sciences2015/Kirkwood_2015_Deciphering death.pdf}
}

@article{gompertzXXIVNatureFunction1825,
  title = {{{XXIV}}. {{On}} the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies. {{In}} a Letter to {{Francis Baily}}, {{Esq}}. {{F}}. {{R}}. {{S}}. \&c},
  author = {Gompertz, Benjamin},
  year = {1825},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {115},
  pages = {513--583},
  publisher = {{Royal Society}},
  doi = {10.1098/rstl.1825.0026},
  urldate = {2023-03-01},
  abstract = {Dear Sir, The frequent opportunities I have had of receiving pleasure from your writings and conversation, have induced me to prefer offering to the Royal Society through your medium, this Paper on Life Contingencies, which forms part of a continuation of my original paper on the same subject, published among the valuable papers of the Society, as by passing through your hands it may receive the advantage of your judgment.},
  file = {/home/yuji/Dropbox/Zotero/Philosophical Transactions of the Royal Society of London1997/Gompertz_1997_XXIV.pdf}
}

@inproceedings{hamonEcoevolutionaryDynamicsNonepisodic2023,
  title = {Eco-Evolutionary {{Dynamics}} of {{Non-episodic Neuroevolution}} in {{Large Multi-agent Environments}}},
  booktitle = {Proceedings of the {{Companion Conference}} on {{Genetic}} and {{Evolutionary Computation}}},
  author = {Hamon, Gautier and Nisioti, Eleni and {Moulin-Frier}, Cl{\'e}ment},
  year = {2023},
  month = jul,
  series = {{{GECCO}} '23 {{Companion}}},
  pages = {143--146},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3583133.3590703},
  urldate = {2024-02-24},
  abstract = {Neuroevolution (NE) has recently proven a competitive alternative to learning by gradient descent in reinforcement learning tasks. However, the majority of NE methods and associated simulation environments differ crucially from biological evolution: the environment is reset to initial conditions at the end of each generation, whereas natural environments are continuously modified by their inhabitants; agents reproduce based on their ability to maximize rewards within a population, while biological organisms reproduce and die based on internal physiological variables that depend on their resource consumption; simulation environments are primarily single-agent while the biological world is inherently multi-agent and evolves alongside the population. In this work we present a method for continuously evolving adaptive agents without any environment or population reset. The environment is a large grid world with complex spatiotemporal resource generation, containing many agents that are each controlled by an evolvable recurrent neural network and locally reproduce based on their internal physiology. The entire system is implemented in JAX, allowing very fast simulation on a GPU. We show that NE can operate in an ecologically-valid non-episodic multi-agent setting, finding sustainable collective foraging strategies in the presence of a complex interplay between ecological and evolutionary dynamics.},
  isbn = {9798400701207},
  file = {/home/yuji/Dropbox/Zotero/Association for Computing Machinery/2023/Hamon et al_2023_Eco-evolutionary Dynamics of Non-episodic Neuroevolution in Large Multi-agent.pdf}
}

@article{bredecheEmbodiedEvolutionCollective2018,
  title = {Embodied {{Evolution}} in {{Collective Robotics}}: {{A Review}}},
  shorttitle = {Embodied {{Evolution}} in {{Collective Robotics}}},
  author = {Bredeche, Nicolas and Haasdijk, Evert and Prieto, Abraham},
  year = {2018},
  journal = {Frontiers in Robotics and AI},
  volume = {5},
  issn = {2296-9144},
  urldate = {2023-02-23},
  abstract = {This article provides an overview of evolutionary robotics techniques applied to online distributed evolution for robot collectives, namely, embodied evolution. It provides a definition of embodied evolution as well as a thorough description of the underlying concepts and mechanisms. This article also presents a comprehensive summary of research published in the field since its inception around the year 2000, providing various perspectives to identify the major trends. In particular, we identify a shift from considering embodied evolution as a parallel search method within small robot collectives (fewer than 10 robots) to embodied evolution as an online distributed learning method for designing collective behaviors in swarm-like collectives. This article concludes with a discussion of applications and open questions, providing a milestone for past and an inspiration for future research.},
  file = {/home/yuji/Dropbox/Zotero/Frontiers in Robotics and AI2018/Bredeche et al_2018_Embodied Evolution in Collective Robotics.pdf}
}

@article{uchibeFindingIntrinsicRewards2008,
  title = {Finding Intrinsic Rewards by Embodied Evolution and Constrained Reinforcement Learning},
  author = {Uchibe, Eiji and Doya, Kenji},
  year = {2008},
  month = dec,
  journal = {Neural Networks},
  series = {{{ICONIP}} 2007},
  volume = {21},
  number = {10},
  pages = {1447--1455},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2008.09.013},
  urldate = {2023-02-23},
  abstract = {Understanding the design principle of reward functions is a substantial challenge both in artificial intelligence and neuroscience. Successful acquisition of a task usually requires not only rewards for goals, but also for intermediate states to promote effective exploration. This paper proposes a method for designing `intrinsic' rewards of autonomous agents by combining constrained policy gradient reinforcement learning and embodied evolution. To validate the method, we use Cyber Rodent robots, in which collision avoidance, recharging from battery packs, and `mating' by software reproduction are three major `extrinsic' rewards. We show in hardware experiments that the robots can find appropriate `intrinsic' rewards for the vision of battery packs and other robots to promote approach behaviors.},
  langid = {english}
}

@article{elfwingEmergencePolymorphicMating2014,
  title = {Emergence of {{Polymorphic Mating Strategies}} in {{Robot Colonies}}},
  author = {Elfwing, Stefan and Doya, Kenji},
  year = {2014},
  month = apr,
  journal = {PLOS ONE},
  volume = {9},
  number = {4},
  pages = {e93622},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0093622},
  urldate = {2023-02-23},
  abstract = {Polymorphism has fascinated evolutionary biologists since the time of Darwin. Biologists have observed discrete alternative mating strategies in many different species. In this study, we demonstrate that polymorphic mating strategies can emerge in a colony of hermaphrodite robots. We used a survival and reproduction task where the robots maintained their energy levels by capturing energy sources and physically exchanged genotypes for the reproduction of offspring. The reproductive success was dependent on the individuals' energy levels, which created a natural trade-off between the time invested in maintaining a high energy level and the time invested in attracting mating partners. We performed experiments in environments with different density of energy sources and observed a variety in the mating behavior when a robot could see both an energy source and a potential mating partner. The individuals could be classified into two phenotypes: 1) forager, who always chooses to capture energy sources, and 2) tracker, who keeps track of potential mating partners if its energy level is above a threshold. In four out of the seven highest fitness populations in different environments, we found subpopulations with distinct differences in genotype and in behavioral phenotype. We analyzed the fitnesses of the foragers and the trackers by sampling them from each subpopulation and mixing with different ratios in a population. The fitness curves for the two subpopulations crossed at about 25\% of foragers in the population, showing the evolutionary stability of the polymorphism. In one of those polymorphic populations, the trackers were further split into two subpopulations: (strong trackers) and (weak trackers). Our analyses show that the population consisting of three phenotypes also constituted several stable polymorphic evolutionarily stable states. To our knowledge, our study is the first to demonstrate the emergence of polymorphic evolutionarily stable strategies within a robot evolution framework.},
  langid = {english},
  file = {/home/yuji/Dropbox/Zotero/PLOS ONE2014/Elfwing_Doya_2014_Emergence of Polymorphic Mating Strategies in Robot Colonies.pdf}
}

@inproceedings{elfwingBiologicallyInspiredEmbodied2005,
  title = {Biologically Inspired Embodied Evolution of Survival},
  booktitle = {2005 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {Elfwing, S. and Uchibe, E. and Doya, K. and Christensen, H.I.},
  year = {2005},
  month = sep,
  volume = {3},
  pages = {2210-2216 Vol. 3},
  issn = {1941-0026},
  doi = {10.1109/CEC.2005.1554969},
  abstract = {Embodied evolution is a methodology for evolutionary robotics that mimics the distributed, asynchronous and autonomous properties of biological evolution. The evaluation, selection and reproduction are carried out by and between the robots, without any need for human intervention. In this paper, we propose a biologically inspired embodied evolution framework, which fully integrates self-preservation, recharging from external batteries in the environment, and self-reproduction, pair-wise exchange of genetic material, into a survival system. The individuals are explicitly evaluated for the performance of the battery capturing task, but also implicitly for the mating task by the fact that an individual that mates frequently has larger probability to spread its gene in the population. We have evaluated our method in simulation experiments and the simulation results show that the solutions obtained by our embodied evolution method were able to optimize the two survival tasks, battery capturing and mating, simultaneously. We have also performed preliminary experiments in hardware, with promising results.}
}

@article{singhWhereRewardsCome,
  title = {Where {{Do Rewards Come From}}?},
  author = {Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  abstract = {Reinforcement learning has achieved broad and successful application in cognitive science in part because of its general formulation of the adaptive control problem as the maximization of a scalar reward function. The computational reinforcement learning framework is motivated by correspondences to animal reward processes, but it leaves the source and nature of the rewards unspecified. This paper advances a general computational framework for reward that places it in an evolutionary context, formulating a notion of an optimal reward function given a fitness function and some distribution of environments. Novel results from computational experiments show how traditional notions of extrinsically and intrinsically motivated behaviors may emerge from such optimal reward functions. In the experiments these rewards are discovered through automated search rather than crafted by hand. The precise form of the optimal reward functions need not bear a direct relationship to the fitness function, but may nonetheless confer significant advantages over rewards based only on fitness.},
  langid = {english},
  file = {/home/yuji/Dropbox/Zotero/_/Singh et al_Where Do Rewards Come From.pdf}
}

@article{watsonEmbodiedEvolutionDistributing2002,
  title = {Embodied {{Evolution}}: {{Distributing}} an Evolutionary Algorithm in a Population of Robots},
  shorttitle = {Embodied {{Evolution}}},
  author = {Watson, Richard A. and Ficici, Sevan G. and Pollack, Jordan B.},
  year = {2002},
  month = apr,
  journal = {Robotics and Autonomous Systems},
  volume = {39},
  number = {1},
  pages = {1--18},
  issn = {0921-8890},
  doi = {10.1016/S0921-8890(02)00170-7},
  urldate = {2023-02-23},
  abstract = {We introduce Embodied Evolution (EE) as a new methodology for evolutionary robotics (ER). EE uses a population of physical robots that autonomously reproduce with one another while situated in their task environment. This constitutes a fully distributed evolutionary algorithm embodied in physical robots. Several issues identified by researchers in the evolutionary robotics community as problematic for the development of ER are alleviated by the use of a large number of robots being evaluated in parallel. Particularly, EE avoids the pitfalls of the simulate-and-transfer method and allows the speed-up of evaluation time by utilizing parallelism. The more novel features of EE are that the evolutionary algorithm is entirely decentralized, which makes it inherently scalable to large numbers of robots, and that it uses many robots in a shared task environment, which makes it an interesting platform for future work in collective robotics and Artificial Life. We have built a population of eight robots and successfully implemented the first example of Embodied Evolution by designing a fully decentralized, asynchronous evolutionary algorithm. Controllers evolved by EE outperform a hand-designed controller in a simple application. We introduce our approach and its motivations, detail our implementation and initial results, and discuss the advantages and limitations of EE.},
  langid = {english},
  file = {/home/yuji/Dropbox/Zotero/Robotics and Autonomous Systems2002/Watson et al_2002_Embodied Evolution.pdf}
}

@article{hintonHowLearningCan1987,
  title = {How {{Learning Can Guide Evolution}}},
  author = {Hinton, Geoffrey E and Nowlan, Steven J},
  year = {1987},
  abstract = {The assumption that acquired characteristics are not inherited is often taken to imply that the adaptations that an organism learns during its lifetime cannot guide the course of evolution. This inference is incorrect (Baldwin, 1896). Learning alters the shape of the search space in which evolution operates and thereby provides good evolutionary paths towards sets of co-adapted alleles. We demonstrate that this effect allows learning organisms to evolve much faster than their nonlearning equivalents, even though the characteristics acquired by the phenotype are not communicated to the genotype.},
  langid = {english},
  file = {/home/yuji/Dropbox/Zotero/undefined/undefined/Hinton_Nowlan_How Learning Can Guide Evolution.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  file = {/home/yuji/Dropbox/Zotero/The MIT Press2018/Sutton_Barto_2018_Reinforcement learning.pdf}
}

@article{schultzNeuronalRewardDecision2015,
  title = {Neuronal {{Reward}} and {{Decision Signals}}: {{From Theories}} to {{Data}}},
  shorttitle = {Neuronal {{Reward}} and {{Decision Signals}}},
  author = {Schultz, Wolfram},
  year = {2015},
  month = jul,
  journal = {Physiological Reviews},
  volume = {95},
  number = {3},
  pages = {853--951},
  publisher = {{American Physiological Society}},
  issn = {0031-9333},
  doi = {10.1152/physrev.00023.2014},
  urldate = {2022-04-29},
  abstract = {Rewards are crucial objects that induce learning, approach behavior, choices, and emotions. Whereas emotions are difficult to investigate in animals, the learning function is mediated by neuronal reward prediction error signals which implement basic constructs of reinforcement learning theory. These signals are found in dopamine neurons, which emit a global reward signal to striatum and frontal cortex, and in specific neurons in striatum, amygdala, and frontal cortex projecting to select neuronal populations. The approach and choice functions involve subjective value, which is objectively assessed by behavioral choices eliciting internal, subjective reward preferences. Utility is the formal mathematical characterization of subjective value and a prime decision variable in economic choice theory. It is coded as utility prediction error by phasic dopamine responses. Utility can incorporate various influences, including risk, delay, effort, and social interaction. Appropriate for formal decision mechanisms, rewards are coded as object value, action value, difference value, and chosen value by specific neurons. Although all reward, reinforcement, and decision variables are theoretical constructs, their neuronal signals constitute measurable physical implementations and as such confirm the validity of these concepts. The neuronal reward signals provide guidance for behavior while constraining the free will to act.},
  file = {/home/yuji/Dropbox/Zotero/Physiological Reviews2015/Schultz_2015_Neuronal Reward and Decision Signals.pdf}
}

@article{berridgeAffectiveNeurosciencePleasure2008,
  title = {Affective Neuroscience of Pleasure: Reward in Humans and Animals},
  shorttitle = {Affective Neuroscience of Pleasure},
  author = {Berridge, Kent C. and Kringelbach, Morten L.},
  year = {2008},
  month = aug,
  journal = {Psychopharmacology},
  volume = {199},
  number = {3},
  pages = {457--480},
  issn = {1432-2072},
  doi = {10.1007/s00213-008-1099-6},
  urldate = {2022-10-29},
  abstract = {Pleasure and reward are generated by brain circuits that are largely shared between humans and other animals.},
  langid = {english},
  file = {/home/yuji/Dropbox/Zotero/Psychopharmacology2008/Berridge_Kringelbach_2008_Affective neuroscience of pleasure.pdf}
}

