\section{Introduction}\label{sec:intro}
Reward is a fundamental signal that shapes our behavior through reinforcement learning (RL).
Positive rewards encourage us to eat food and find mating partners, while negative rewards, such as pain and fatigue, help us protect ourselves.

% Problem: Reward evolution
% Why this topic has been underexplored, and how we can find some interesting points from this line of research
%In addition to RL, how the system for producing rewards has evolved is an interesting open question to understand our reward system. A common explanation is based on natural selection, arguing that
Reward signals should have evolved to help animals survive and reproduce offspring (e.g., by~\cite{schultzNeuronalRewardDecision2015}), but how the variety or rewards, such as smells of foods or vision of s partners, evolved remains unclear.
%, although some neuroscientists guess external threats and the need for hunting could be infuential~\citep{ledouxSoonThereWas2022a}.

% Aim
This paper addresses this question from a computational perspective.
Because it is difficult to observe the biological process of evolution of reward signals, we develop a simplified model of the birth and death of RL agents in a 2D environment and examine what types of rewards evolve in a
% Why the population-based model is important
%For this purpose, we propose to use a population-based
distributed evolution framework.
Previous studies on the evolution of learning \citep{hintonHowLearningCan1987,singhWhereRewardsCome2009} employed a centralized evolution scheme where all agents in the population are ranked and elites are selected as parents.
% This approach is simple and computationally efficient but sacrifices the biological reality. Instead,
Our simulation model employs birth and death conditions depending on the energy level and the age. Each agent inherits a reward function from its parent and learns to get more rewards by RL during its lifetime. %Through this process, we expect that naturalistic reward functions will evolve.

We implement this framework using JAX\citep{jax2018github} to accelerate simulation by utilizing GPU. Our simulation results show that the evolution of food reward is surprisingly unstable due to space and food resource constraints. In our qualitative analysis, we found that

We show that the sparseness of the population and foods can be an important factor in the evolution of stable food rewards.

\section{Preliminaries and Related Works}\label{sec:related}
We follow the standard computational RL framework \citep{suttonReinforcementLearningIntroduction2018} based on the Markov decision process (MDP). MDP $\M{}$ consists of a tuple $(\X{}, \A{}, p, r, \gamma)$, where $\X{}$ is state space, $\A{}$ is action space, $p: \X\times\A\times \X \rightarrow [0, 1]$ is state transition function, $r: \X\times\A \rightarrow \mathcal{R}$ is reward function, and $\gamma \in [0, 1]$ is the discount factor. A standard objective in MDP is the discounted cumulative return $G \defequal{} \sum_{t=0}^{\infty}\gamma^t R_{t}$, where $R_t$ is the reward received at time $t$. An RL agent has policy $\pi: \X \times \A \rightarrow [0, 1]$ and seeks to find the the optimal policy $\pi^{*}$ that maximizes $\E \left[G|\pi\right]$. The state-value function $V^\pi(s) \defequal \sum_{a \in \A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in \X} P(s'|s, a) V^\pi(s') \right)$ is often used in RL algorithms. Observation $o \in \Omega~(\Omega \subseteq \X)$ often refers to a part of the state that an agent can observe.

Our reward model is inspired by the neuroscience of reward system \citep{schultzNeuronalRewardDecision2015, berridgePleasureSystemsBrain2015}. Notably, \citet{berridgeDissectingComponentsReward2009} argue that the brain reward system consists of three independent components: liking, wanting, and learning. In analogy with computational RL, liking corresponds to reward function, wanting corresponds to learned policy, and learning corresponds to learning state value $V$. %\citet{dayanLikingEarlyEditable2022} discussed the idea that liking is related to reward shaping \citep{ngPolicyInvarianceReward1999} in computational RL that helps an agent to learn faster. This aligns with our view of rewards, although we don't focus on the quality of rewards in this paper.

While
%we are focusing on the distributed evolution of rewards in a multi-agent setting,
there were previous attempts to evolve rewards in a single-agent setting \citep{singhWhereRewardsCome2009,niekumEvolutionRewardFunctions2011,zhengWhatCanLearned2020},
%While evolutionary robotics studies \citep{nolfiEvolutionaryRoboticsBiology2004} often employ a centralized selection scheme similar to genetic algorithm \citep{mitchellIntroductionGeneticAlgorithms1998},
%where highly evaluated elites are selected as parents. On the contrary,
we take a distributed embodied evolution (EE) framework \citep{watsonEmbodiedEvolutionDistributing2002,bredecheEmbodiedEvolutionCollective2018}
%that employs a decentralized evolution without centralized evaluation
where agents evolve locally following birth and death rules. This method has the advantage that the evaluation of genetic traits depends on population dynamics, which is more natural.

Our work is inspired by the series of studies \citep{elfwingBiologicallyInspiredEmbodied2005,elfwingDarwinianEmbodiedEvolution2011,elfwingEmergencePolymorphicMating2014}, which tried to evolve parameters related to RL through EE framework. Notably, \citet{elfwingDarwinianEmbodiedEvolution2011} evolved supplementary sharing rewards and parameters of RL agents.
%, and \citet{elfwingEmergencePolymorphicMating2014} evolved agents with a hierarchical RL mechanism, where the higher module chooses either foraging or mating, and the lower module outputs more primitive actions.
Inspired by these works, we attempt to evolve all reward functions in this experiment.
%instead of hyperparameters.

%Among them, \citet{singhWhereRewardsCome2009} pioneered the idea that rewards should be internal to agents from the RL community.
\citet{zhengWhatCanLearned2020} also inspired us to try a large-scale evolutionary simulation of rewards utilizing hardware accelerators.

\section{Simulation Model and Environment}\label{sec:method}

% \begin{figure}[t]
%   \centering{}
%   \includegraphics[width=15cm]{hazard_and_survival.pdf}
%   \caption{
%     \textbf{Left:} Designed hazard function $h(t)$ used for RL agents.
%     \textbf{Right:} Survival function $S(t)$ corresponding to the hazard function.
%   }\label{figure:hs}
% \end{figure}

\paragraph{Energy-based death and birth model}
We employ an energy-based model similar to \citet{hamonEcoevolutionaryDynamicsNonepisodic2023}. Each agent maintains their energy level $e$,
which increases by eating food and decreases by the basal metabolism and taking a motor action.
We design the agent's death and birth model
% to maintain a higher energy level $e$, which leads to longer lives and many offspring. With
based on $e$ and an agent's age $t$.
We adopt a Gompertz hazard model \citep{gompertzXXIVNatureFunction1825,kirkwoodDecipheringDeathCommentary2015}, in which the probability of an agent dying in a time step is given by:
\begin{align}
  h(t, e) = \frac{\kappa_{h}}{1 + \alpha_{e}\exp(\beta_{he}e)} + \alpha_{t} \exp(\beta_{ht} t).
  \label{eq:h}
\end{align}
The first term increases as energy levels decrease following a sigmoidal curve where the scale $\kappa_{h}$ is the scale, $\beta_{he}$ is the slope, and $\alpha_{e}$ defines the shape when $e=0$.
The latter term exponentially increases as the agent gets older with the cale $\alpha_{t}$ and the slope $\beta_{ht}$.
The left panel in \Cref{figure:hs} shows the shape of the hazard function with the specific parameters used in our experiments.
The right panel in \Cref{figure:hs} shows the survival function $S(t, e) = \exp (-\int_{0}^{t}(h(t, e)) dt)$, the probability for an agent to survive to the age $t$ if it keeps the same energy level $e$.
We can see that the survival probability more sharply decays with aging when the energy level is low.

\begin{figure}[t]
  \centering{}
  \includegraphics[width=15cm]{birth_and_nc.pdf}
  \caption{
    The left figure shows the designed birth rate $b(e)$ for RL agents, the center figure shows the expected lifetime for each agent, and the right figure shows the expected reproduction number $R$ for RL agents corresponding to the designed hazard and birth functions.
  }\label{figure:bnc}
\end{figure}

For simplicity, we employ an asexual reproduction model \cref{eq:h} with the birth function, the probability for an agent with energy level $e$ to produce a child in a time step:
\begin{align}
 b(e) &= \frac{\kappa_{b}}{1 + \exp(\beta_{b}e)}.
 \label{eq:b}
\end{align}
%Modeled as a generalized logistic function \citep{richardsFlexibleGrowthFunction1959},
The birth rate $b(e)$ increases with $e$ following a sigmoidal curve where $\kappa_{b}$ is the scale and $\beta_{b}$ is the slope.
The left figure of \Cref{figure:bnc} shows the shape of $b$ for the parameters used in the experiment.
Based on the birth model $b(e)$ and the death model $h(e,t)$, we plot the expected lifetime and number of children if an agent keeps the same energy level in the center and right figure in \cref{figure:bnc}. Parameters are chosen so that the maximum expected lifetime is around \num{200000} and the maximum expected number of children is around 40. All experiment parameters are shown in \cref{ap:param}.

\paragraph{Environment}

\begin{figure}[t]
  \begin{subfigure}[t]{6cm}
    \centering
    \includegraphics[width=6cm]{emevo-ss.png}
    \subcaption{Simulation environment we used in our experiments.}
  \end{subfigure}
  \begin{subfigure}[t]{8cm}
    \centering
    \includegraphics[width=8cm]{emevo-anno.png}
    \subcaption{Description of the environment}
  \end{subfigure}
  \caption{
    In the right figure, thin gray lines around agents indicate distance that can detect the distance to the closest object. Red circles indicate food. Outer gray lines are walls.
    In the right figure, An agent can move by adding force to two indicated points.
  }\label{figure:env}
\end{figure}

%As a simple simulation environment with biologically plausible sensorimotor interaction, w
We designed a continuous 2D environment shown in \Cref{figure:env}. Blue circles indicate agents, and red circles indicate foods. The environment is implemented by a 2D rigid-body physics simulation, where agents can move by
%adding motor outputs
producing driving forces on the left and right sides of the body.
%at two points in the diagonal back part of the circle.
%We use these two motor outputs as an action of agent in RL.
An agent has multiple range sensors in its front, which can sense the type and the distance to the closest object (food, agent, and wall) within $120$ degree.

An agent can eat food by touching it and gain energy $e_{\mathrm{food}}$. After foods are eaten, they are regenerated in a random place.
% This eat-driven emergence is not consistent with the growth model below. Better report the actual food producing process rather than theoretical approximation.
The rate of food regeneration follows logistic growth function $\frac{dN_{food}}{dt} = g N_{food} (1 - \frac{N_{food}}{N_{food}^{\mathrm{max}}}$, where $N_{food}$ is the number of food, $g$ is growth rate, and $N_{food}^{\mathrm{max}}$ is the maximum number of food.
Eating food increases an agent's internal energy by $e_{\mathrm{food}}$. In some experiments, we use poisonous food that decreases energy level by $e_{\mathrm{poison}}$.

When an agent makes a child, a new agent is placed in a random location sampled from a Gaussian distribution centered around its parent. %However, to speed up the simulation, r
Reproduction fails when all
% report how many times.
sampled locations are not available. Thus, keeping away from the wall and other agents is beneficial for agents to make more children. When a child is produced
%the parent's energy level is $e$,
a proportion of the parent's energy $\eta e \in [0, 1]$ is given to the child, and the parent's energy level decreases to $(1-\eta)e$.
%, where $\eta \in [0, 1]$ is the ratio of energy sharing.
The child also inherits its reward function from the parent with some mutation.

%Simulating many agents to maintain a reasonable size population (e.g., $50\sim 100$) is critical in our evolution scheme.
To speed up the simulation, we implement our environment using JAX Python library \citep{jax2018github} to utilize GPU or other hardware accelerators. Inspired by recent works on 3D rigid body physics simulation using JAX (e.g., \citet{brax2021github} and MuJoCo \citep{todorov2012mujoco} MJX\footnote{\url{https://mujoco.readthedocs.io/en/stable/mjx.html}}), we implement our 2D physics engine using JAX and build our environment on top of that, optimizing it for multi-agent setting. Our simulator implements projected Gauss-Seidel method with position correction \citep{catto2005iterative} that is pretty common in 2D game physics engines such as Box2D\footnote{\url{https://box2d.org}} and Chipmunk\footnote{\url{https://chipmunk-physics.net}}.

\paragraph{Reward Function with Evolving Weights} %Parameters}
We assume that the reward function is determined at birth with mutation and does not change during an agent's lifetime. As inputs to the reward function, we consider \rnum{1} collision to food (eating), \rnum{2} collision to the wall, \rnum{3} collision to some other agent, and \rnum{4} the magnitude of agent's action (motor output). In experiments with poisonous food, the collision with food is also included.
We expect that a positive reward for food evolves to acquire energy
and a negative reward for action evolves to save energy. Collisions with the wall and other agents are chosen to test if the reward function can distinguish unrelevant signals for survival, as we do not penalize agents for colliding with walls or each other.

We take a linear model of the reward function with weights for each sensory input
\begin{align}
  r = \sum_i r_i = \sum_i w_i f_i
  \label{eq:reward}
\end{align}
where $w_i$ is the weight for a sensorimotor event $f_i$ for $i\in{\mbox{\{foood, wall, agent,  action\}}}$.
% This redundant coding is better put in Appendix
%For example, the reward weight for getting food is modeled as $r_{\mathrm{food}} = 10^{s_{\mathrm{food}}} w_{\mathrm{food}}$, where $s_{\mathrm{food}} \in [0, 1]$ and $w_{\mathrm{food}} \in [-1, 1]$ are evolvable parameters that represent scale and weight for the food reward weight.
%We model reward weights for other sensory inputs by $r_\mathrm{wall}$, $r_\mathrm{agent}$, and $r_\mathrm{agent}$ in the same way.
%Our intention in using two independent parameters $s$ and $w$ is to balance stability and plasticity. Our preliminary experiments found that rewards can be too unstable if we allow significant change via a single parameter. Having a parameter for adjusting the scale separately makes the significant reward change more challenging while still possible. Reward weights are multiplied by collision events or action magnitude and then summed up to the reward that an agent uses in RL.
We explain the details of implementation in \cref{ap:reward}.

% Add a line for reward acquisition; after observation or action?
\begin{algorithm}
  \caption{Reward evolution with asexual reproduction}\label{alg:reward-evo}
  \begin{tabular}{lll}
    \textbf{Input:} & $Pop$ & Initial population of agents \\
                    & $Env$ & Simulation environment \\
                    & $h, b$ & Hazard and birth functions \\
                    & $m$ & mutation function \\
                    & $N$ & Rollout step used in RL \\
                    & $\eta$ & Energy share ratio
  \end{tabular}
  \begin{algorithmic}[1]
    \Loop{}
    \LComment{Interact with environment}
    \ForAll{$agent \in Pop$}
      \State{$o \gets agent$'s observation in $Env$}
      \State{$a \sim agents$'s policy $\pi_{agent}(\cdot|o)$}
      \State{Update $agent$'s energy level}
      \Once{in $N$ steps}
        \State{Update $agent$'s policy $\pi_{agent}$ via RL}
      \EndOnce{}
    \EndFor{}
    \State{Step $Env$ using collected actions}
    \LComment{Process birth and death}
    \ForAll{$agent \in Pop$ with energy $e$ and age $t$}
      \With{Probability $b(e)$} \Comment{Birth}
        \State{Update $agent$'s energy to $(1 - \eta) e$}
        \State{$r \gets agent$'s reward function}
        \State{Create a new agent with $\eta e$ and $m(r)$}
      \EndWith{}
      \With{Probability $h(t, e)$} \Comment{Death}
        \State{$agent$ is removed from $Pop$ and $Env$}
      \EndWith{}
    \EndFor{}
  \EndLoop{}
\end{algorithmic}
\end{algorithm}

\paragraph{Simulation Procedure}
We show the pseudocode of our entire simulation procedure in \cref{alg:reward-evo}. Each agent has its innate, immutable reward function and learnable neural network policy. At each step, it observes sensory inputs from the environment and takes action. Once in $N$ steps, the agent updates its policy via RL using the past $N$ step experiences. We use Proximal Policy Optimization \citep{schulmanProximalPolicyOptimization2017} as an RL algorithm because of its fast computation time. We explain the details of our RL implementation in appendix. After environmental interaction, all agents can make a child and die based on the birth function $b$ and hazard function $h$. The new agent inherits a fraction of the parent's energy and mutated reward function. As mutation, we add uniform noise to each $s$ and $w$ with probability $0.4$. We show some relevant parameters in \cref{ap:param}.

\section{Results}

\begin{figure}[t]
  \begin{subfigure}[t]{5cm}
    \centering
    \includegraphics[width=5cm]{baseline.png}
    \subcaption{Evolved rewards.}\label{subfigure:bl}
  \end{subfigure}
  \begin{subfigure}[t]{5cm}
    \centering
    \includegraphics[width=5cm]{seed5-frdecay.png}
    \subcaption{Decreasing food reward.}\label{subfigure:s5}
  \end{subfigure}
  \begin{subfigure}[t]{5cm}
    \centering
    \includegraphics[width=5cm]{seed2-frstable.png}
    \subcaption{Stable food reward.}\label{subfigure:s2}
  \end{subfigure}
  \caption{
    In the left figure, the X-axis shows the birth time of each agent, and the y-axis shows the value of rewards for food, agent, wall, and action from top to bottom. The difference in colors and shapes of dots corresponds to the difference in the random seed. In the center and right figures, reward weights are colorized per food reward weight. The center figure shows the case where food reward decreases (seed $5$), and the right figure shows one with stable food reward,  from \num{4e6} to \num{9e6} steps.
  }\label{figure:baseline-result}
\end{figure}

As a baseline environment, we use the setting where food is always regenerated in the upper right of the room, as the left figure in \Cref{figure:env} shows. In this environment, we tuned environmental parameters (e.g., maximum number of foods and growth rate) to stabilize the population around $100$. We show all environmental parameters in \cref{ap:param}. All experiments start with $50$ agents that have energy level $40.0$ and randomly initialized reward weights. We conducted \num{1e7} step of the simulation, resulting from $300$ to $400$ generations, which takes $14\sim16$ hours on NVIDIA P100 GPUs.

\Cref{subfigure:bl} shows the evolved reward weights with 5 random seeds in the baseline environment. Surprisingly, the food reward weight is only stable positive for one random seed. It unstably goes up and down or stays negative in one random seed. On the contrary, reward weights for colliding with walls and other agents are stably negative, which is reasonable because sticking around walls is not good for foraging, and sticking to other agents might reduce the chance of making a child. The reward weight for action tends to be positive. This contradicts our expectations that it would evolve into negative, like fatigue.

To investigate the reason why the evolution of food reward can be unstable, we closely look at our simulation run with seed $5$, where food reward decreases. \Cref{subfigure:s5} shows the scatter plot of reward weights in this run from \num{4e6} to \num{9e6} steps, where blue color indicates smaller food rewards and red color indicates larger. We can see a tendency that agents with lower food rewards tend to have higher wall and action rewards, which is reasonable because both sticking around walls and just moving can help foraging. On the contrary, we can see two different modes in agent and action rewards. In the medium of \cref{subfigure:s5} (between \num{6e6} and \num{7e6} steps), agents with smaller rewards tend to have smaller negative rewards for colliding with an agent, while they tend to have larger agent rewards later. On the contrary, in the case where the food reward remains stable, shown in \cref{subfigure:s2} (seed $2$), the magnitude of other collision rewards (agent and wall) is relatively small, and they don't look have a large effect. The reward for action increases over the course of evolution, which might help foraging with food rewards.

\begin{figure}[t]
  \begin{subfigure}[t]{7cm}
    \centering
    \includegraphics[width=7cm]{seed5-lh-orange.png}
    \subcaption{Life history of agents in Fig. 4b.}\label{subfigure:s5lh}
  \end{subfigure}
  \begin{subfigure}[t]{7cm}
    \centering
    \includegraphics[width=7cm]{seed2-lh-orange.png}
    \subcaption{Life history of agents in Fig. 4c.}\label{subfigure:s2lh}
  \end{subfigure}
  \caption{
    Agent's life history. The left figure corresponds to the period filled with orange in \cref{subfigure:s5}, and the right figure corresponds to the period filled with orange in \cref{subfigure:s2}. Lines show the trajectories of agents, and dots show places where agents made a child. Colors correspond to negative and positive food rewards.
  }\label{figure:s5-lh}
\end{figure}

To investigate the difference in the two cases, we focus on the period we filled orange in the first row of \cref{subfigure:s5} and \cref{subfigure:s2}, where both negative and positive food rewards coexist. We plot agents' life history in this period in \cref{figure:s5lh}, where thin lines show the agents' trajectories, dots show places where agents got children, and different colors correspond to the value of food rewards. In both figures, we can see that agents' trajectories are concentrated around the food location, while they make children in more distributed places because of the environmental constraint that makes it more difficult to have a child in a crowded area.
Although we don't see significant differences in agents' trajectories, we can see a tendency that agents with negative food rewards are distributed more distant from the food rewards because of the smaller negative food reward in \cref{subfigure:s2lh}. On the contrary, we guess that relatively larger negative food rewards in \cref{subfigure:s5lh} are not very harmful, especially with stronger rewards for collision with agents and walls. Instead, it looks like the negative rewards for colliding each other helps agents to form a collective behavior that doesn't look very directed.

\begin{figure}[t]
  \begin{subfigure}[t]{6cm}
    \centering
    \includegraphics[width=6cm]{comp_baseline_linep2.png}
    \subcaption{Comparison of evolved rewards between food locations settings.}\label{subfigure:blcomp}
  \end{subfigure}
  \begin{subfigure}[t]{4.5cm}
    \centering
    \includegraphics[width=4.5cm]{lv-s1.png}
    \subcaption{Unstable food reward in \texttt{spread food} setting.}\label{subfigure:lv1}
  \end{subfigure}
  \begin{subfigure}[t]{4.5cm}
    \centering
    \includegraphics[width=4.5cm]{season-s4.png}
    \subcaption{Unstable food reward in \texttt{food reloc.} setting.}\label{subfigure:season4}
  \end{subfigure}
  \caption{
    The left figure shows evolved rewards in multiple settings, including the baseline environment.
    Each line shows the reward weight averaged over all agents alive at that time.
    Different colors indicate different settings.
    In the center and right figures, reward weights are colorized per food reward weight, as in \cref{figure:s5-lh}. The center figure show the case where food reward is unstable in \texttt{spread food} setting, and the right one shows one in \texttt{food reloc} setting.
    \label{figure:foodloc-comp}
  }
\end{figure}

In the baseline environment, agents are densely located in the food location, which seems to influence the evolution of rewards. For example, negative rewards for food or another agent would help them move to sparser locations where they are more likely to produce offspring. Therefore, we conduct another experiment with more distributed food regeneration (\texttt{spread foods} in the figure) and moving food location after eating $1000$ foods (\texttt{food reloc.} in the figure). \Cref{subfigure:blcomp} compares the evolved reward functions with the baseline. To make the comparison easier, we average the reward weight of all available agent at a certain step, and plot the mean for $5$ runs each. Although food rewards can be unstable with all these three settings, the \texttt{spread foods} setting is the most stable in the sense that food reward is stable in 3 runs within 5 random seeds. Reward weights for colliding with walls and other agents are almost always negative, as in the baseline setting. The reward for action is generally positive, but sometimes goes negative with \texttt{spread foods} setting, producing the population that sticks to the wall.

\begin{figure}[t]
  \begin{subfigure}[t]{7cm}
    \centering
    \includegraphics[width=7cm]{lv-lh-orange.png}
    \subcaption{A case study for decreasing food reward.}\label{subfigure:lvlh}
  \end{subfigure}
  \begin{subfigure}[t]{7cm}
    \centering
    \includegraphics[width=7cm]{season-lh-orange.png}
    \subcaption{A case study for decreasing food reward.}\label{subfigure:seasonlh}
  \end{subfigure}
  \caption{
    Agent's life history. The left figure corresponds to the period filled with orange in \cref{subfigure:s5}, and the right figure corresponds to the period filled with orange in \cref{subfigure:s2}. Lines show the trajectories of agents, and dots show places where agents made a child. Colors correspond to negative and positive food rewards.
  }\label{figure:s5-lh}
\end{figure}

\section{Conclusion}
In this paper, we propose a distributed evolutionary simulation to investigate the evolution of reward function. Our simulation results show that the evolution of food reward is often unstable under many conditions because of food regeneration and spatial constraints. We show that the sparseness of the population and foods can be an important factor in the evolution of stable food rewards.