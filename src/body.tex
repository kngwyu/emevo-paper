\section{Introduction}
Producing reward signal is a fundamental brain function for animals to live and reproduce. Positive rewards such as pleasure encourages us to eat and find partners, while negative rewards such as fatigue help us protect ourselves. These reward signals affect our behavior through learning. Our brain produce reward signal to a certain stimulus such as good food, and the signal is used to reinforce our behavior that lead to the reward. This mechanism is called reinforcement learning (RL) and has been extensively studied in both neuroscience and computer science. Neuroscientists have revealed that our brain has some hot or cold spots that respond to good or bad events~\citep{berridgeAffectiveNeurosciencePleasure2008}, and how those signals are used for learning~\citep{schultzNeuronalRewardDecision2015}, while computational reinforcement learning has provided theoretical models and several applications including game-playing agents and robotics~\citep{suttonReinforcementLearningIntroduction2018}.

% Problem: Reward evolution
% Why this topic has been underexplored, and how we can find some interesting points from this line of research
Due to its significant contribution to our lives, how our reward system have evolved is an interesting open question. A common explanation is based on natural selection, arguing that rewards have evolved to help animals survive in the environment and successfully reproduce offspring (e.g., by~\cite{schultzNeuronalRewardDecision2015}). This hypothesis sounds reasonable, because our reward functions such as one for food reward are actually helping us to live longer and contribute to our fitness. However, there remain some questions in the detailed process of reward evolution. Given that some animals, including ascidians, have evolved to have evolved to abandon brain, the conditions under which reward system is required is not obvious. Another question is how the mechanism for suppressing rewards evolved. For example, rewards for food are suppressed by satiety. It is not obvious how such suppression mechanisms have evolved along with rewards.

% Importance of simulation
While biological studies on both evolutionary biology and neuroscience are highly important to address these questions, we argue that computational study can also contribute to this area by providing a simplified model of real evolution. A good example is the relationship between biological and computational RL, which have evolved closely. While computational RL has provided us with useful engineering techniques, it has also provided a promising computational model for how animals learn. We aim to make a similar contribution to the problem of reward evolution.

% Why population-based model is important
For this purpose, what kind of simulation model is suitable? Natural selection is the idea that genetic traits that contribute to more offspring will survive and evolve. Indicators such as the average number of births that assess this reproductive success are also called fitness. Some classical studies on the evolution of learning \citep{hintonHowLearningCan1987} have conducted simulations by explicitly designing a fitness function for a particular task. This approach has an advantage that evaluation of genetic traits is computational cheap, whereas to evaluate genetic traits from reproductive success alone would require simulations over several generations in a given population. However, this method has a disadvantage that the design of the fitness function is not always natural because of the designer's bias. Since we are interested in how biologically plausible rewards evolve, we prefer to use more natural measures such as reproductive success. Furthermore, today, we have much greater computational resources that enable larger populations are possible.

% Proposed simulation model
To this end, we propose to use a distributed evolutionary model of reward function, inspired by the concept posed by embodied evolution (EE)~\cite{watsonEmbodiedEvolutionDistributing2002}. While a major line of work in evolutionary computation considers evolution with a designed fitness function, EE aims to simulate evolution in a fully distributed way with particular death and birth conditions.

an ecologically-plausible environment to conduct simulation.

% Result

\section{Related Work}\label{sec:2}
RL community has contributed to \citet{singhWhereRewardsCome} is also important.

Therefore, \citet{watsonEmbodiedEvolutionDistributing2002} proposed a scheme called embodied evolution (EE). In EE, all the evaluation, reproduction, and selection are performed by individual robots in a decentralized manner, while traditional ER employs a centralized selection similar to GA\@. Robots die based on a defined lifespan and mortality rate, and newly born ones replace the programs of the dead robots.

The series of studies by \citet{elfwingBiologicallyInspiredEmbodied2005,elfwingDarwinianEmbodiedEvolution2011a,elfwingEmergencePolymorphicMating2014} drew the main inspiration to this work.
They tried to evolve shaping rewards and meta parameters of reinforcement learning agents using mobile robots called Cyber Rodents. In particular, \citet{elfwingDarwinianEmbodiedEvolution2011a} evolved reward-shaping parameters of RL agents, showing that rewards that make learning easier had evolved, while \citet{elfwingEmergencePolymorphicMating2014} evolved agents with a hierarchical RL mechanism, where the higher module chooses either foraging or mating, and the lower module outputs more primitive actions. They evolved only the higher decision-making parameters, while the lower modules were learned by RL. In their experiments, they observed two different groups: one that favors mating behavior and the other that favors foraging behavior. In our study, we try to evolve primary rewards instead of shaping rewards or the hierarchical decision-making mechanism they use. \citet{uchibeFindingIntrinsicRewards2008} also experimented with a similar mobile robot where primitive rewards such as food were predefined, but intrinsic rewards such as curiosity were evolved. In contrast, our study attempts to evolve primitive rewards. A drawback of EE is that reproduction is often done by replacing the genome of the same robot \citep{bredecheEmbodiedEvolutionCollective2018}. This makes it challenging to reproduce population increases and decreases by simulation. While the Darwinian EE framework proposed by \citet{elfwingDarwinianEmbodiedEvolution2011a} partially addressed this by having multiple agents as a virtual population inside a robot, we allow for population increase and decrease in simulation.

\section{Simulation Model and Environment}\label{sec:3}

\paragraph{Energy-based birth and death model}
We assume that each agent has its own energy level $e$ and age $t$. We let $b(e)$ the birth function that evaluates the probability that an agents give birth to its child. Similarly, we let $h(t, e)$ be the hazard function for agents that evaluates the probability for an agent to die. Note that we only consider asexual reproduction model and our agents don't have sex, so all agents are evaluated whether they can make child. We design $b$ and $h$ as follows:

\begin{align}
  b(e) &=  \frac{\kappa_{b}}{1 + \alpha_{b}\exp(d_{b} - e)}, \label{eq:b} \\
  h(t, e) &= \kappa_{h} \left(1 - \frac{1}{1 + \alpha_{he} \exp(d_{h} - e)} \right) + \alpha_{ht} \exp(\beta t). \label{eq:h}
\end{align}

Modeled as a generalized logistic function \citep{richardsFlexibleGrowthFunction1959}, the birth function\label{eq:b} means that birth probability increases with $e$ following a sigmoidal curve where $\kappa_{b}$ is the scale, $\d_{b}$ controls the delay of reaction, and $\alpha_{e}$ defines the shape of when $e=d_{b}$. The hazard function\label{eq:h} consists of two terms. The first term increases as energy levels decrease follows a sigmoidal curve similar to the birth function, where $\kappa_{h}, d_{h}$, and $\alpha_{he}$ are hyper parameters that define the shape. The latter term $\alpha_{ht} \exp(\beta t)$ exponentially increases when the agent gets older, where $\alpha_{ht}$ and $\beta$ are hyper parameters. This is called Gompertz hazard model\citep{gompertzXXIVNatureFunction1825,kirkwoodDecipheringDeathCommentary2015} in population statistics.

\paragraph{Environment}

\paragraph{Reward Model}
As a reward function, we used a simple linear function $\mathbf{r} = \left(r_{\textrm{agent}}, r_{\textrm{food}}, r_{\textrm{wall}}, r_{\textrm{action}} \right)^{\intercal} $ for an agent's collision and action. Let $c_{t}^{\textrm{agent}}, c_{t}^{\textrm{food}}$, and $c_{t}^{\textrm{wall}}$ be binary variables representing the agent's collision with other agents, food, and walls and obstacles at time $t$. In addition, let $J_{t}^{\textrm{norm}} = \frac{|J_{t}|}{\max{|J|}}$ denote the $[0, 1]-$normalized impulse an agent adds to itself at time $t$. Then, $\mathbf{r}$ maps these four values to a single scalar reward by $r = \mathbf{r}^{\intercal} (c_{t}^{\textrm{agent}}, c_{t}^{\textrm{food}}, c_{t}^{\textrm{wall}}, J_{t}^{\textrm{norm}})^{\intercal}$. $\mathbf{r}$ is the only genetically inheritable trait for the agent, with noise added by mutation at a certain probability. I may use a more complex reward function in future experiments, but for this preliminary experiment, I preferred simplicity. Mutation gives noise sampled from a mixture of binominal and Gaussian distribution. Specifically, uniform noise $[-0.1, 0.1]$ is added with probability $0.4$ to each element of $\mathbf{r}$.

So what is an appropriate functional form for $h$ and $b$? $h$ should decrease with increasing $e$, and $b$ should increase, but at the same time, I want to avoid too steep changes, which could lead to extinction and population explosion. Thus, I modeled $h$ and $b$ with a generalized logistic function \citep{richardsFlexibleGrowthFunction1959} as follows:

I assume that each agent already knows how to learn from rewards. I used PPO\citep{schulmanProximalPolicyOptimization2017} as an RL algorithm because of its fast simulation performance. To estimate $A^{\pi}$, GAE (\cref{eq:gae}) is used with $N=1024$, so an agent learns only once every $1024$ step. As a neural network function approximation, I used a simple linear network shown by \cref{eq:v-nn} with additional layers for modeling $\pi$. Following the common practice in the literature, I modeled $\pi(\cdot|\textbf{s})$ as independent Gaussian distributions for each $ (J, \theta_{J}, \frac{d\theta_{\textrm{ang}}}{dt}) $ with state-independent variance. Other RL-related parameters are put in \cref{tab:rl-param}.

$r = r_{\textrm{agent}}c_{t}^{\textrm{agent}} + r_{\textrm{food}}c_{t}^{\textrm{food}} + r_{\textrm{wall}}c_{t}^{\textrm{wall}} + r_{\textrm{action}}J_{t}^{\textrm{norm}}$

\begin{algorithm}
  \caption{Pseudo code of reward evolution with asexual reproduction}\label{alg:reward-evo}
  \begin{tabular}{lll}
    \textbf{Input:} & $Pop$ & Initial population \\
                    & $Env$ & Simulated environment \\
                    & $N$ & Number of rollout steps used in RL \\
                    & $h(s, e)$ & Hazard function for agents \\
                    & $b(e)$ & Birth function for agents \\
  \end{tabular}
  \begin{algorithmic}[1]
    \Loop{}
    \ForAll{$agent \in Pop$}
      \State{$s \gets agent$'s observation in $Env$}
      \State{Sample an action $a$ from $agent$'s policy $\pi_{agent}(\cdot|s)$}
      \State{Update $agent$'s energy level $e$ based on the taken action $a$ and eaten food}
      \Once{in $N$ steps}
        \State{Update $agent$'s policy $\pi_{agent}$ and value function $\vh_{agent}$ via RL}
      \EndOnce{}
    \EndFor{}

    \State{Step the simulated environment $Env$ one step using collected actions}
    \LComment{Process death and birth}
    \ForAll{$agent \in Pop$ whose energy level is $e$ and age is $t$}
      \With{Probability $h(s, e)$}
        \State{$agent$ is removed from $Pop$ and $Env$}
        \Comment{agent is dead}
      \EndWith{}
      \With{Probability $b(e)$}
        \State{$\mathbf{r}_{\textrm{new}} \gets$ $agent$'s $\mathbf{r}~ + $ sampled noise}
        \State{Add a new agent with $\mathbf{r}_{\textrm{new}}$ to $Pop$ and $Env$}
      \EndWith{}
    \EndFor{}
  \EndLoop{}
\end{algorithmic}
\end{algorithm}

To summarize, I show the whole simulation process of reward evolution with asexual reproduction \Cref{alg:reward-evo}. Sexual reproduction may be considered in future studies, but only asexual reproduction was considered in this preliminary experiment. Basically, each agent acts based on their policy, and they occasionally update their policy via RL. They consume energy by acting and learning but can get energy by eating food. At each simulation step, an agent dies with probability $h(t, e)$ and creates a child with probability $b(e)$. The new agent inherits the reward function of its parent but may also acquire a new one via mutation. The implementation is done via Jax framework\citep{jax2018github} in Python to benefit from vectorization on both CPU and GPU. Further thread-level parallelization may be considered in future research.

\section{Results}

\section{Conclusion}