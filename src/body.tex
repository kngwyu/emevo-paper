\section{Introduction}
Producing reward signal is a fundamental brain function for animals to live and reproduce. Positive rewards such as pleasure encourages us to eat and find partners, while negative rewards such as fatigue help us protect ourselves. These reward signals affect our behavior through learning. Our brain produce reward signal to a certain stimulus such as good food, and the signal is used to reinforce our behavior that lead to the reward. This mechanism is called reinforcement learning (RL) and has been extensively studied in both neuroscience and computer science. Neuroscientists have revealed that our brain has some hot or cold spots that respond to good or bad events~\citep{berridgeAffectiveNeurosciencePleasure2008}, and how those signals are used for learning~\citep{schultzNeuronalRewardDecision2015}, while computational reinforcement learning has provided theoretical models and several applications including game-playing agents and robotics~\citep{suttonReinforcementLearningIntroduction2018}.

% Problem: Reward evolution
% Why this topic has been underexplored, and how we can find some interesting points from this line of research
Due to its significant contribution to our lives, how our reward system have evolved is an interesting open question. A common explanation is based on natural selection, arguing that rewards have evolved to help animals survive in the environment and successfully reproduce offspring (e.g., by~\cite{schultzNeuronalRewardDecision2015}). This hypothesis sounds reasonable, because our reward functions such as one for food reward are actually helping us to live longer and contribute to our fitness. However, there remain some questions in the detailed process of reward evolution. Given that some animals, including ascidians, have evolved to have evolved to abandon brain, the conditions under which reward system is required is not obvious. Another question is how the mechanism for suppressing rewards evolved. For example, rewards for food are suppressed by satiety. It is not obvious how such suppression mechanisms have evolved along with rewards.

% Importance of simulation
While biological studies on both evolutionary biology and neuroscience are highly important to address these questions, we argue that computational study can also contribute to this area by providing a simplified model of real evolution. A good example is the relationship between biological and computational RL, which have evolved closely. While computational RL has provided us with useful engineering techniques, it has also provided a promising computational model for how animals learn. We aim to make a similar contribution to the problem of reward evolution.

% Why population-based model is important
For this purpose, what kind of simulation model is suitable? Natural selection is the idea that genetic traits that contribute to more offspring will survive and evolve. Indicators such as the average number of births that assess this reproductive success are also called fitness. Some classical studies on the evolution of learning \citep{hintonHowLearningCan1987} have conducted simulations by explicitly designing a fitness function for a particular task. This approach has an advantage that evaluation of genetic traits is computational cheap, whereas to evaluate genetic traits from reproductive success alone would require simulations over several generations in a given population. However, this method has a disadvantage that the design of the fitness function is not always natural because of the designer's bias. Since we are interested in how biologically plausible rewards evolve, we prefer to use more natural measures such as reproductive success. Furthermore, today, the available computational resources are much greater than they were then, and simulations of somewhat larger populations are possible.

% Proposed simulation model
To this end, we propose an evolutionary simulation model of reward function.

embodied evolution (EE)~\cite{watsonEmbodiedEvolutionDistributing2002}.
In the broader literature of evolutionary robotics, EE b

Since our aim is to confirm the environmental effects to reward functions in a biologically plausible way, EE is a good choice.


% Result

\section{Related Work}
My work is mainly inspired by the series of studies by \citet{elfwingBiologicallyInspiredEmbodied2005,elfwing
  DarwinianEmbodiedEvolution2011a,elfwingEmergencePolymorphicMating2014}. They tried to evolve shaping rewards and meta parameters of reinforcement learning agents using mobile robots called \textit{Cyber Rodents}. In particular, \citet{elfwingDarwinianEmbodiedEvolution2011a} evolved reward-shaping parameters of RL agents, showing that rewards that make learning easier had evolved. Furthermore, \citet{elfwingEmergencePolymorphicMating2014} evolved agents with a hierarchical RL mechanism, where the higher module chooses either foraging or mating, and the lower module acts based on, the higher choice (i.e., foraging or mating) and the current state. They evolved only the higher decision-making parameters, while the lower modules were learned by RL\@. In their experiments, they observed two different groups: one that favors mating behavior and the other that favors foraging behavior. In our study, we try to evolve primary rewards instead of shaping rewards or the hierarchical decision-making mechanism they use. \citet{uchibeFindingIntrinsicRewards2008} also experimented with a similar mobile robot where primitive rewards such as food were predefined, but intrinsic rewards such as curiosity were evolved. In contrast, our study attempts to evolve primitive rewards. A drawback of EE is that reproduction is often done by replacing the genome of the same robot \citep{bredecheEmbodiedEvolutionCollective2018}. This makes it challenging to reproduce population increases and decreases by simulation. While the Darwinian EE framework proposed by \citet{elfwingDarwinianEmbodiedEvolution2011a} partially addressed this by having multiple agents as a virtual population inside a robot, we allow for population increase and decrease in simulation.

\section{Evolutionary Model}
Simulating evolution using the fitness function has some problems, as discussed in \cref{ch1-fitness-or-population}. Therefore, in this study, I employ an EE approach that does not use a fitness function. While most EE studies assume a fixed number of robots due to hardware limitations, my preliminary experiments only consider simulation where population change is much easier. So, I used a simple setting that all agents share the same hazard rate $h$ and birth rate $b$. I expect $h$ and $b$ to increase as agents learn foraging behavior, thereby increasing their lifespan, the number of offspring, and the expected reproduction number $R$. Therefore, I assigned each agent an energy state $e$, which increases or decreases depending on the agent's behavior. Eating food increases $e$, but learning or moving consumes energy and decreases $e$. Using $e$, I modeled hazard function $h (t,e)$ and $b (e)$, where $t$ is the age of an agent. Note that $b$ also can depend on $t$, but I employed a more straightforward form in my preliminary experiments.

So what is an appropriate functional form for $h$ and $b$? $h$ should decrease with increasing $e$, and $b$ should increase, but at the same time, I want to avoid too steep changes, which could lead to extinction and population explosion. Thus, I modeled $h$ and $b$ with a generalized logistic function \citep{richardsFlexibleGrowthFunction1959} as follows:
\begin{align}
  h(t, e) &= - \kappa_{h} \frac{1}{1 + \alpha_{he}\exp( - e)} + h_{\textrm{age}}(t), \\
  b(e) &=  \kappa_{b} \frac{1}{1 + \alpha_{b}\exp(d_{b} - e)},
\end{align}
where $\kappa_{h}$ is the scaling parameter for hazard function, $\alpha_{he}$ is the inverse initial hazard that determines the behavior when $e$ is enough small, $\kappa_{b}$ is the scaling parameter for birth function, $\alpha_{e}$ is the inverse initial value of the birth function, and $d_{b}$ is the delay parameters that work as thresholds for $e$. $h_{\textrm{age}}(t)$ is an age-dependent term of hazard. Using Gompertz model\citep{gompertzXXIVNatureFunction1825,kirkwoodDecipheringDeathCommentary2015}, I modeled $h_{\textrm{age}}$ by $h_{\textrm{age}} = \alpha_{ht} \exp(\beta t)$. In other words, the hazard function increases exponentially over time, with $\alpha_{ht}$ controlling $h(0)$ and $\beta$ controlling the scale. While the Gompertz hazard function is not realistic for human populations because it does not capture the high mortality rate of newborns \citep{makehamLawMortalityConstruction1860}, the simplicity of this model has led to its widespread use from biological modeling to industrial product failure rates.


\section{Results}

\section{Conclusion}