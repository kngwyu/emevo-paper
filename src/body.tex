\section{Introduction}

Producing reward signal is a fundamental brain system for animals to live and reproduce. Positive rewards such as pleasure encourages us to eat and find partners, while negative rewards such as fatigue are useful for protecting ourselves. An important function of the reward signal is to give a direction to our learning and behavior. Our brain produces reward signal to a certain stimulus such as good food, and the signal is used to reinforce the behavior that leads to the reward. Due to its importance, the learning based on reward, called reinforcement learning (RL), has been extensively studied in both neuroscience and computer science. Neuroscientists have revealed that our brain has some hot or cold spots that respond to good or bad events~\cite{berridgeAffectiveNeurosciencePleasure2008}, and how those signals are used for learning~\cite{schultzNeuronalRewardDecision2015}, while computational reinforcement learning has provided theoretical models and several applications including game-playing agents and robotics.

However, compared to RL, less attention has been paid to the reward itself. Why we have such rewards, what is the common traits, and how did we acquired it in the course of evolution? This may be partly because the role played by the reward system in evolution seems obvious. A common explanation is based on natural selection, arguing that rewards have evolved to help animals survive in the environment and successfully reproduce offspring (e.g., by~\cite{schultzNeuronalRewardDecision2015}). While this hypothesis is reasonable, there remain some questions in the detailed process of reward evolution. An important questions include in what environmental conditions rewards are important. For example, in an environment where animals can get enough nutrition without doing anything, maybe they don't need any reward for food. Thus, the need of foraging and competition with other individuals can make food rewards important, but their effect is not obvious. \todo{One more question}

To answer these questions, we propose an embodied evolution (EE)~\cite{watsonEmbodiedEvolutionDistributing2002} model of reward functions. While real experiments of evolution and phylogenetic analysis are highly important, it is still not plausible to conduct extensive evolutionary experiments using animals with brains complex enough to have reward systems. Thus, computer simulation might be a good choice in that it can enable us simulate the relatively long course of evolution, while we need to sacrifice some kind of reality. Since our aim is to confirm the environmental effects to reward functions in a biologically plausible way, EE is a good coice.

We implemented our model by JAX.

\section{Related Work}
My work is mainly inspired by the series of studies by \citet{elfwingBiologicallyInspiredEmbodied2005,elfwingDarwinianEmbodiedEvolution2011a,elfwingEmergencePolymorphicMating2014}. They tried to evolve shaping rewards and meta parameters of reinforcement learning agents using mobile robots called \textit{Cyber Rodents}. In particular, \citet{elfwingDarwinianEmbodiedEvolution2011a} evolved reward-shaping parameters of RL agents, showing that rewards that make learning easier had evolved. Furthermore, \citet{elfwingEmergencePolymorphicMating2014} evolved agents with a hierarchical RL mechanism, where the higher module chooses either foraging or mating, and the lower module acts based on, the higher choice (i.e., foraging or mating) and the current state. They evolved only the higher decision-making parameters, while the lower modules were learned by RL\@. In their experiments, they observed two different groups: one that favors mating behavior and the other that favors foraging behavior. In our study, we try to evolve primary rewards instead of shaping rewards or the hierarchical decision-making mechanism they use. \citet{uchibeFindingIntrinsicRewards2008} also experimented with a similar mobile robot where primitive rewards such as food were predefined, but intrinsic rewards such as curiosity were evolved. In contrast, our study attempts to evolve primitive rewards. A drawback of EE is that reproduction is often done by replacing the genome of the same robot \citep{bredecheEmbodiedEvolutionCollective2018}. This makes it challenging to reproduce population increases and decreases by simulation. While the Darwinian EE framework proposed by \citet{elfwingDarwinianEmbodiedEvolution2011a} partially addressed this by having multiple agents as a virtual population inside a robot, we allow for population increase and decrease in simulation.