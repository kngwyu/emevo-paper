\section{Introduction}\label{sec:intro}
Producing reward signals is a fundamental animal brain function to live and reproduce. Positive rewards, such as pleasure, encourage us to eat and find partners, while negative rewards, such as fatigue, help us protect ourselves. These reward signals affect our behavior through learning. Our brain produces a reward signal to a certain stimulus, such as good food, reinforcing our behavior to get more rewards. This mechanism, called reinforcement learning (RL), has been extensively studied in neuroscience and computer science. Neuroscientists have revealed that our brain has some hot or cold spots that respond to good or bad events~\citep{berridgeAffectiveNeurosciencePleasure2008}, and how those signals are used for learning~\citep{schultzNeuronalRewardDecision2015}, while computational reinforcement learning has provided theoretical models and several applications including game-playing agents and robotics~\citep{suttonReinforcementLearningIntroduction2018}.

% Problem: Reward evolution
% Why this topic has been underexplored, and how we can find some interesting points from this line of research
Due to its significant contribution to our lives, how our reward system has evolved is an interesting open question. A common explanation is based on natural selection, arguing that rewards have evolved to help animals survive in the environment and successfully reproduce offspring (e.g., by~\cite{schultzNeuronalRewardDecision2015}). While this hypothesis sounds reasonable, it is unclear what kind of environmental condition triggered the evolution of rewards. Another question is how the characteristics of our current rewards have evolved, such as food reward regulation by satiety.

% Importance of simulation
While biological studies on evolutionary biology and neuroscience are highly important in addressing these questions, we argue that computational study can contribute to this area by providing a simplified model of real evolution. A good example is the relationship between biological and computational RL, which have evolved closely. While computational RL has provided us with useful engineering techniques, it has also provided a promising computational model for how animals learn. We aim to make a similar contribution to the problem of reward evolution.

% Why the population-based model is important
For this purpose, we propose to use a population-based distributed evolution framework to simulate the evolution of rewards. Classical studies on the evolution of learning \citep{hintonHowLearningCan1987,singhWhereRewardsCome2009} often employ a centralized evolution scheme where some tightly evaluated elites are selected as parents. While this approach is simple and computationally efficient, it somewhat sacrifices the biological reality. Instead, our simulation model employs birth and death conditions for each agent and lets evolution happen locally. Each agent inherits a reward function from its parent and learns to get more rewards during its lifetime. Through this process, we expect that naturalistic reward functions will evolve.

\section{Preliminaries and Related Works}\label{sec:related}
We follow the standard computational RL framework \citep{suttonReinforcementLearningIntroduction2018} based on Markov decision process (MDP). MDP $\M{}$ consists of a tuple $(\X{}, \A{}, p, r, \gamma)$, where $\X{}$ is state space, $\A{}$ is action space, $p: \X\times\A\times \X \rightarrow [0, 1]$ is state transition function, $r: \X\times\A \rightarrow \mathcal{R}$ is reward function, and $\gamma \in [0, 1]$ is the discount factor. A standard objective in MDP is the discounted cumulative return $G \defequal{} \sum_{t=0}^{\infty}\gamma^t R_{t}$, where $R_t$ is the reward received at time $t$. An RL agent has policy $\pi: \X \times \A \rightarrow [0, 1]$ and seeks to find the the optimal policy $\pi^{*}$ that maximizes $\E \left[G|\pi\right]$. The state-value function $V^\pi(s) \defequal \sum_{a \in \A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in \X} P(s'|s, a) V^\pi(s') \right)$ is often used in RL algorithms. Observation $o \in \Omega~(\Omega \subseteq \X)$ often refers to a part of the state that an agent can observe.

Our reward model is inspired by the neuroscience of reward system \citep{schultzNeuronalRewardDecision2015, berridgePleasureSystemsBrain2015}. Notably, \citet{berridgeDissectingComponentsReward2009} argue that the brain reward system consists of three independent components: liking, wanting, and learning. In analogy with computational RL, liking corresponds to reward function, wanting corresponds to learned policy, and learning corresponds to learning state value $V$. \citet{dayanLikingEarlyEditable2022} discussed the idea that liking is related to reward shaping \citep{ngPolicyInvarianceReward1999} in computational RL that helps an agent to learn faster. This aligns with our view of rewards, although we don't focus on the quality of rewards in this paper.

Our distributed evolutionary simulation framework shares lots of concepts with embodied evolution (EE) \citep{watsonEmbodiedEvolutionDistributing2002,bredecheEmbodiedEvolutionCollective2018}. While evolutionary robotics \citep{nolfiEvolutionaryRoboticsBiology2004} usually employs a centralized selection scheme similar to genetic algorithm \citep{mitchellIntroductionGeneticAlgorithms1998}, where highly evaluated elites are selected as parents,
reproduction and selection are performed locally in a decentralized manner in EE.

Our work is especially inspired by the series of studies \citep{elfwingBiologicallyInspiredEmbodied2005,elfwingDarwinianEmbodiedEvolution2011,elfwingEmergencePolymorphicMating2014}, which tried to evolve parameters related to RL through EE framework. Notably, \citet{elfwingDarwinianEmbodiedEvolution2011} evolved reward-shaping parameters of RL agents, and \citet{elfwingEmergencePolymorphicMating2014} evolved agents with a hierarchical RL mechanism, where the higher module chooses either foraging or mating, and the lower module outputs more primitive actions. Inspired by these works, we attempt to evolve innate reward functions instead of hyperparameters.

While we are focusing on the distributed evolution of rewards in a multi-agent setting, there were some previous attempts to evolve rewards for a specific task in a single-agent setting \citep{singhWhereRewardsCome2009,niekumEvolutionRewardFunctions2011,zhengWhatCanLearned2020}. Among them, \citet{singhWhereRewardsCome2009} was pioneering in suggesting the idea that rewards should be internal to agents from the RL community, and \citet{zhengWhatCanLearned2020} was inspiring to us in that they first tried large-scale evolutionary simulation of rewards utilizing hardware accelerators.

\section{Simulation Model and Environment}\label{sec:method}

\begin{figure}[t]
  \centering{}
  \includegraphics[width=15cm]{hazard_and_survival.pdf}
  \caption{
    \textbf{Left:} Designed hazard function $h(t)$ used for RL agents.
    \textbf{Right:} Survival function $S(t)$ corresponding to the hazard function.
  }\label{figure:hs}
\end{figure}

\paragraph{Energy-based death and birth model}
As a simple but biologically plausible way of simulating the birth and death of agents, we employ an energy-based model similar to \citet{hamonEcoevolutionaryDynamicsNonepisodic2023}. Each agent maintains their energy level $e$. Energy level $e$ increases by eating and decreases by basic metabolism and taking action in the environment. We design the agent's death and birth model to maintain a higher energy level $e$ leads to longer lives and many offspring. With $e$ and an agent's age $t$, we let $h(t, e)$ be the hazard function for agents that evaluates the probability of an agent dying:

\begin{align}
  h(t, e) = \kappa_{h} \left(1 - \frac{1}{1 + \alpha_{he} \exp(d_{h} - e)} \right) + \alpha_{ht} \exp(\beta t). \label{eq:h}
\end{align}

This hazard function\label{eq:h} consists of two terms. The first term increases as energy levels decrease and follow a sigmoidal curve, where $\kappa_{h}, d_{h}$, and $\alpha_{he}$ are hyperparameters. The latter term $\alpha_{ht} \exp(\beta t)$ exponentially increases when the agent gets older, where $\alpha_{ht}$ and $\beta$ are hyperparameters. In population statistics, this is called the Gompertz hazard model\citep{gompertzXXIVNatureFunction1825,kirkwoodDecipheringDeathCommentary2015}. The left figure in \cref{figure:hs} shows the shape of the hazard function with the specific parameters used in our experiments. To intuitively understand the behavior $h$, we plot survival function $S(t, e) = \exp (-\int_{0}^{t}(h(t, e)) dt)$ in the right of \cref{figure:hs}. $S(t, e)$ is the probability for an agent to survive to the age $t$ if it keeps the same energy level $e$. We can see that the survival probability more sharply decays with aging when the energy level is low.

\begin{figure}[t]
  \centering{}
  \includegraphics[width=15cm]{birth_and_nc.pdf}
  \caption{
    \textbf{Left:} Designed birth rate $b(e)$ for RL agents.
    \textbf{Center:} Expected lifetime for each agent
    \textbf{Right:} Expected reproduction number $R$ for RL agents corresponding to the designed hazard and birth functions.
  }\label{figure:bnc}
\end{figure}

For simplicity, we employ an asexual reproduction model where all agents can have a chance to make their children. We let $b(e)$ the birth function that evaluates the probability for an agent with energy level $e$ to make its child. Similarly to the first term of \cref{eq:h}, we design the birth function $b$ as:
\begin{align}
 b(e) &=  \frac{\kappa_{b}}{1 + \alpha_{b}\exp(d_{b} - e)}. \label{eq:b}
\end{align}
Modeled as a generalized logistic function \citep{richardsFlexibleGrowthFunction1959}, $b(e)$ increases with $e$ following a sigmoidal curve where $\kappa_{b}$ is the scale, $d_{b}$ controls the delay of reaction, and $\alpha_{e}$ defines the shape of when $e=d_{b}$. The left figure of \cref{figure:bnc} shows the shape of $b$. For chosen $b$ and $h$, we plot the expected lifetime and number of children when an agent keeps the same energy level in the center and right figure in \cref{figure:bnc}. Parameters are chosen so that the maximum expected lifetime is around \num{200000} and the maximum expected number of children is around 40. All parameters used in experiments are shown in \cref{ap:param}.

\paragraph{Environment}

\begin{figure}[t]
  \begin{subfigure}[t]{6cm}
    \centering
    \includegraphics[width=6cm]{emevo-ss.png}
  \end{subfigure}
  \begin{subfigure}[t]{8cm}
    \centering
    \includegraphics[width=8cm]{emevo-anno.png}
  \end{subfigure}
  \caption{
    \textbf{Left:} Environment.
    \textbf{Right:} Annotation
  }\label{figure:env}
\end{figure}

As a simple simulation environment yet with biologically plausible sensorimotor interaction, we design a continuous 2D environment shown in \cref{figure:env}. Blue circles indicate agents, and red circles indicate food. The environment is implemented by a 2D rigid-body physics simulation, and agents can move by adding motor outputs at two points in the diagonal back part of the circle. An agent can eat food by touching it. After foods are eaten, they are regenerated in a random place. The rate of food regeneration follows logistic growth function $\frac{dN_{food}}{dt} = r N_{food} (1 - \frac{N_{food}}{N_{food}^{\mathrm{max}}}$, where $N_{food}$ is the number of food, $r$ is growth rate, and $N_{food}^{\mathrm{max}}$ is the maximum number of food. Eating food increases

When an agent makes a child, a new agent is placed in a random location sampled from a Gaussian distribution centered around its parent. The new agent inherits its reward function from its parent with some mutation, as described later. To avoid overlapping objects, we sample multiple random locations at one time and choose one that does not conflict with any other object. However, to speed up the simulation, reproduction fails when all sampled locations are not available. Thus, keeping away from other agents is beneficial for agents to make more children. When an agent dies, its body is immediately removed from the environment.

Simulating many agents to maintain a reasonable size population (e.g., $50\sim 100$) is key in our evolution scheme. However, in our preliminary experiments using CPU-based simulation, we found that simulating collisions between many agents and foods could be a huge bottleneck in computation time. To overcome this challenge, we implement our environment using JAX Python library \citep{jax2018github} so that it can work on hardware accelerators such as GPU. Inspired by recent works on 3D rigid body physics simulation using JAX (e.g., \citet{brax2021github} and MuJoCo \citep{todorov2012mujoco} MJX\footnote{\url{https://mujoco.readthedocs.io/en/stable/mjx.html}}), we implement our own 2D physics engine using JAX and build our environment on top of that. This design decision is made because many of the existing JAX-based physics simulation libraries are optimized for single-agent robot control and not well suited for handling the multi-agent scenario we are dealing with. We explain implementation detail in \cref{ap:phys}.

\paragraph{Reward Model}
We assume that the reward function is innate and cannot be changed by learning during an agent's lifetime. As sensory inputs that can affect reward function, we select \rnum{1} collision to food, \rnum{2} collision to the wall, \rnum{3} collision to some other agent, and \rnum{4} the magnitude of motor output.


As a reward function, we used a simple linear function $\mathbf{r} = \left(r_{\textrm{agent}}, r_{\textrm{food}}, r_{\textrm{wall}}, r_{\textrm{action}} \right)^{\intercal} $ for an agent's collision and action. Let $c_{t}^{\textrm{agent}}, c_{t}^{\textrm{food}}$, and $c_{t}^{\textrm{wall}}$ be binary variables representing the agent's collision with other agents, food, and walls and obstacles at time $t$. In addition, let $J_{t}^{\textrm{norm}} = \frac{|J_{t}|}{\max{|J|}}$ denote the $[0, 1]-$normalized impulse an agent adds to itself at time $t$. Then, $\mathbf{r}$ maps these four values to a single scalar reward by $r = \mathbf{r}^{\intercal} (c_{t}^{\textrm{agent}}, c_{t}^{\textrm{food}}, c_{t}^{\textrm{wall}}, J_{t}^{\textrm{norm}})^{\intercal}$. $\mathbf{r}$ is the only genetically inheritable trait for the agent, with noise added by mutation at a certain probability. I may use a more complex reward function in future experiments, but for this preliminary experiment, I preferred simplicity. Mutation gives noise sampled from a mixture of binominal and Gaussian distribution. Specifically, uniform noise $[-0.1, 0.1]$ is added with probability $0.4$ to each element of $\mathbf{r}$.

\begin{algorithm}
  \caption{Pseudo code of reward evolution with asexual reproduction}\label{alg:reward-evo}
  \begin{tabular}{lll}
    \textbf{Input:} & $Pop$ & Initial population \\
                    & $Env$ & Simulated environment \\
                    & $N$ & Number of rollout steps used in RL \\
                    & $h(s, e)$ & Hazard function for agents \\
                    & $b(e)$ & Birth function for agents \\
  \end{tabular}
  \begin{algorithmic}[1]
    \Loop{}
    \ForAll{$agent \in Pop$}
      \State{$s \gets agent$'s observation in $Env$}
      \State{Sample an action $a$ from $agent$'s policy $\pi_{agent}(\cdot|s)$}
      \State{Update $agent$'s energy level $e$ based on the taken action $a$ and eaten food}
      \Once{in $N$ steps}
        \State{Update $agent$'s policy $\pi_{agent}$ and value function $\vh_{agent}$ via RL}
      \EndOnce{}
    \EndFor{}

    \State{Step the simulated environment $Env$ one step using collected actions}
    \LComment{Process death and birth}
    \ForAll{$agent \in Pop$ whose energy level is $e$ and age is $t$}
      \With{Probability $h(s, e)$}
        \State{$agent$ is removed from $Pop$ and $Env$}
        \Comment{agent is dead}
      \EndWith{}
      \With{Probability $b(e)$}
        \State{$\mathbf{r}_{\textrm{new}} \gets$ $agent$'s $\mathbf{r}~ + $ sampled noise}
        \State{Add a new agent with $\mathbf{r}_{\textrm{new}}$ to $Pop$ and $Env$}
      \EndWith{}
    \EndFor{}
  \EndLoop{}
\end{algorithmic}
\end{algorithm}

I assume that each agent already knows how to learn from rewards. I used PPO\citep{schulmanProximalPolicyOptimization2017} as an RL algorithm because of its fast simulation performance. To estimate $A^{\pi}$, GAE (\cref{eq:gae}) is used with $N=1024$, so an agent learns only once every $1024$ step. As a neural network function approximation, I used a simple linear network shown by \cref{eq:v-nn} with additional layers for modeling $\pi$. Following the common practice in the literature, I modeled $\pi(\cdot|\textbf{s})$ as independent Gaussian distributions for each $ (J, \theta_{J}, \frac{d\theta_{\textrm{ang}}}{dt}) $ with state-independent variance. Other RL-related parameters are put in \cref{tab:rl-param}.

$r = r_{\textrm{agent}}c_{t}^{\textrm{agent}} + r_{\textrm{food}}c_{t}^{\textrm{food}} + r_{\textrm{wall}}c_{t}^{\textrm{wall}} + r_{\textrm{action}}J_{t}^{\textrm{norm}}$


To summarize, I show the whole simulation process of reward evolution with asexual reproduction \Cref{alg:reward-evo}. Sexual reproduction may be considered in future studies, but only asexual reproduction was considered in this preliminary experiment. Basically, each agent acts based on their policy, and they occasionally update their policy via RL. They consume energy by acting and learning but can get energy by eating food. At each simulation step, an agent dies with probability $h(t, e)$ and creates a child with probability $b(e)$. The new agent inherits the reward function of its parent but may also acquire a new one via mutation. The implementation is done via Jax framework (e.g., \citet{jax2018github} and MuJoCo) in Python to benefit from vectorization on both CPU and GPU. Further thread-level parallelization may be considered in future research.

\section{Results}

\section{Conclusion}